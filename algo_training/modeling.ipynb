{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gesture Recognition thru Machine Learning\n",
    "========================\n",
    "** Machine learning algorithms applied to glove sensor values to predict gestures. **\n",
    "\n",
    "Consisting of two parts:\n",
    "1. PCA Visualization\n",
    "2. RBF SVM vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "#Essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Libraries for the classifiers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Utilities\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#Model persistence\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Visualization\n",
    "import seaborn as sb\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir_base = \"prediction/\"\n",
    "\n",
    "dir_training = dir_base + \"data/\"\n",
    "dir_scaler = dir_base + \"scaler/\"\n",
    "dir_plots = dir_base + \"plots/\"\n",
    "dir_classifiers = dir_base + \"classifiers/\"\n",
    "\n",
    "# File names\n",
    "consolidated_fn = \"consolidated.csv\"\n",
    "scaler_fn = \"scaler.pkl\"\n",
    "\n",
    "# Cleaning of xlsx data\n",
    "def data_cleaning(file):\n",
    "    feature_names = ['fThumb', 'fIndex', 'fMiddle', 'fRing', 'fPinky', 'c1', 'c2', 'c3', 'c4', 'aX', 'aY', 'aZ', 'gX', 'gY', 'gZ', 'label']\n",
    "    f = pd.read_excel(file, convert_float=True, names=feature_names)\n",
    "    f.dropna(how='any',inplace=True)\n",
    "    f.to_csv(file.split(\".\")[0] + \".csv\", index=False)\n",
    "\n",
    "# Place label at the end\n",
    "def rearrange_cols(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[1:] + cols[:1]\n",
    "    return df[cols]\n",
    "\n",
    "# Cleaning and consolidating of training data\n",
    "def data_consolidate():\n",
    "    for fn in os.listdir(dir_training):\n",
    "        if fn.endswith('.xlsx'):\n",
    "            data_cleaning(dir_training + fn)\n",
    "\n",
    "    data = pd.concat([pd.read_csv(dir_training + fn).groupby('label').median().reset_index() for fn in os.listdir(dir_training) if fn.endswith('.csv')])\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = rearrange_cols(data).sort_values(by='label')\n",
    "    data.to_csv(dir_training+consolidated_fn, index=False)\n",
    "    return data\n",
    "\n",
    "def get_top_five(df):\n",
    "    var_df_flex = df.drop(labels=c_cols+a_cols+g_cols, axis=1)\n",
    "    var_df_flex = var_df_flex.groupby(by=\"label\").std().reset_index().mean(axis=1).sort_values(ascending=False)\n",
    "    print(var_df_flex)\n",
    "    top_five_varied_letters = [target_names[i] for i in var_df_flex[:5].index]\n",
    "    return top_five_varied_letters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### Principal Component Analysis (PCA) \n",
    "Identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_scatter(X_estimator, letter, title, directory):\n",
    "    plt.figure()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(12, 8)\n",
    "    for i in range(len(target_names)):\n",
    "        px = X_estimator[y == i, 0]\n",
    "        py = X_estimator[y == i, 1]\n",
    "        if i is letter :\n",
    "            plt.scatter(px, py, c='red', zorder=2)\n",
    "        else:\n",
    "            plt.scatter(px, py, c='silver', alpha=0.5, zorder=1)\n",
    "        plt.legend(target_names)\n",
    "    plt.title(\"PCA 2 Components plot for letter %s\" % target_names[letter])\n",
    "    plt.xlabel(\"First Component\")\n",
    "    plt.ylabel(\"Second Component\")\n",
    "    plt.savefig(\"%s%s.png\" % (directory, letter))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "### SVM (Support Vector Machine)\n",
    "Obtains hyperplanes, used on separating instances of one class from the rest, in an optimal way by selecting the ones that pass through the widest possible gaps between instances of different classes. New instances will be classified depending on which side of the surface they fall on.\n",
    "\n",
    "### Random Forest\n",
    "Based on bagging, bootstrap aggregation, technique that constructs multitude of randomly trained decision trees in classifying. This is done by obtaining random data subsets from the original dataset and creating decision trees with these subsets. Once the decision trees are constructed, mode of the classifications made by the decision trees will be obtained and treated as its prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(X_train, y_train, X_test, y_test, folds, directory):\n",
    "    #Machine learning algo variables\n",
    "    names = [\n",
    "        \"RBF SVM\",\n",
    "        \"Random Forest\"\n",
    "         ]\n",
    "    classifiers = [\n",
    "        SVC(),\n",
    "        RandomForestClassifier(),\n",
    "        ]\n",
    "\n",
    "    parameters = [\n",
    "        {'kernel': ['rbf','linear'], 'gamma': [1e-3, 1e-4, 2], 'C': [1, 10, 100, 1000]},\n",
    "        {'max_depth': [3, 7, 9, 11, 25], 'n_estimators': [50, 75, 100, 130, 150]},\n",
    "    ]\n",
    "\n",
    "    clf_predictor = []\n",
    "    for name, clf, parameter in zip(names, classifiers, parameters):\n",
    "        print(\"\\n################ %s ################\" % name)\n",
    "        grid_search = GridSearchCV(clf, parameter, cv = folds)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set: \\n %s\" % str(grid_search.best_params_))\n",
    "        print(\"Grid scores on development set: \\n\")\n",
    "        for params, mean_score, scores in grid_search.grid_scores_:\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean_score, scores.std() * 2, params))\n",
    "\n",
    "        print(\"The model is trained on the full development set. \\n The scores are computed on the full evaluation set. \\n\")\n",
    "        y_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "        print(metrics.classification_report(y_true, y_pred))\n",
    "\n",
    "        clf_predictor.append(grid_search)\n",
    "\n",
    "    #Should be in the predictor folder\n",
    "    for clf in clf_predictor:\n",
    "        path = \"%s%s%s.pkl\" % (dir_classifiers, directory, str(clf.estimator).split(\"(\")[0])\n",
    "        joblib.dump(clf, path)\n",
    "        print(\"%s : dumped!\" % str(clf.estimator).split(\"(\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Contains the run/main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Data Parameters\n",
    "target_names = [i for i in string.ascii_uppercase]\n",
    "\n",
    "#file_dir = \"sdf/\"\n",
    "def run(df, directory, cv):\n",
    "    df.to_csv(dir_training + directory + \"data.csv\")\n",
    "    \n",
    "    sl_data = df.iloc[:,:-1]\n",
    "    sl_target = df['label'].apply(lambda x: target_names.index(x)).values\n",
    "\n",
    "    # Data contains the observations, target contains the classifications\n",
    "    X, y = sl_data, sl_target\n",
    "\n",
    "    #Standardize X because of different scaling from different sensors\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X) \n",
    "    print(\"***** Data loaded and standardized *****\")\n",
    "  \n",
    "    # Saving of scaler\n",
    "    joblib.dump(scaler, dir_scaler+directory+scaler_fn)\n",
    "    print(\"***** Scaler dumped *****\")\n",
    "    \n",
    "    ############# PCA ############# \n",
    "    \"\"\"\n",
    "    n = df.columns.size - 1\n",
    "    pca = PCA(n_components=n)\n",
    "    print(\"***** Plotting PCA *****\")\n",
    "\n",
    "    X_pca = pca.fit(X_std).transform(X_std)\n",
    "    for i in range(26):\n",
    "        plot_scatter(X_pca, i, \"PCA of the dataset\", dir_plots + directory)\n",
    "\n",
    "    print('Explained variance ratio (first %s components): %s' % (str(n), str(pca.explained_variance_ratio_)))\n",
    "    \"\"\"\n",
    "    ############# Classifiers ############# \n",
    "    # Assignment of test and training data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.25, random_state=0)\n",
    "    print(\"***** Modeling start *****\")\n",
    "    \n",
    "    classify(X_train, y_train, X_test, y_test, cv, directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir_training+consolidated_fn)\n",
    "cv = 5 # n of folds\n",
    "\n",
    "fs_cols = ['fThumb', 'fIndex', 'fMiddle', 'fRing', 'fPinky']\n",
    "c_cols = ['c1', 'c2', 'c3', 'c4']\n",
    "a_cols = ['aX', 'aY', 'aZ']\n",
    "g_cols = ['gX', 'gY', 'gZ']\n",
    "\n",
    "\n",
    "# df.drop(labels=c_cols+a_cols+g_cols, axis=1, inplace=True)\n",
    "# run(df, \"flex_only/\", cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = joblib.load(dir_classifiers+\"all/SVC.pkl\")\n",
    "scaler = joblib.load(dir_scaler+\"all/scaler.pkl\")\n",
    "\n",
    "svm.predict(scaler.transform(np.array(df.iloc[99:100,:-1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_cross_validation(clf, K):\n",
    "    # create a k-fold croos validation iterator\n",
    "    cv = KFold(len(y_train), K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv)\n",
    "    print(scores)\n",
    "    print((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores)))\n",
    "    \n",
    "def fit_clf(clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def train_and_evaluate(clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    return clf\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# X_KBest = SelectKBest(chi2, k=10).fit_transform(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
